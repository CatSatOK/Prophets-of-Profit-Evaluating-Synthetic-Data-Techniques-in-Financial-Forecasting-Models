{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import GRU, Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy, MeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeGAN Synthetic Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the data\n",
    "real_data = pd.read_csv('../Data/GOOG.csv')   \n",
    "real_data['Date'] = pd.to_datetime(real_data['Date']) #making datetime\n",
    "real_data['Date'] = real_data['Date'].values.astype(float) #converting to float for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters\n",
    "seq_len = 30\n",
    "n_seq = 22 \n",
    "batch_size = 128 \n",
    "\n",
    "#TimeGAN model parameters\n",
    "hidden_dim = 24\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(real_data).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating rolling windows\n",
    "data = []\n",
    "for i in range(len(real_data) - seq_len):\n",
    "    data.append(scaled_data[i:i + seq_len])\n",
    "\n",
    "n_windows = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tf.data.Dataset\n",
    "real_series = (tf.data.Dataset\n",
    "               .from_tensor_slices(data)\n",
    "               .shuffle(buffer_size=n_windows)\n",
    "               .batch(batch_size))\n",
    "real_series_iter = iter(real_series.repeat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up random series generator\n",
    "def make_random_data():\n",
    "    while True:\n",
    "        yield np.random.uniform(low=0, high=1, size=(seq_len, n_seq))\n",
    "\n",
    "random_series = iter(tf.data.Dataset\n",
    "                     .from_generator(make_random_data, output_types=tf.float32)\n",
    "                     .batch(batch_size)\n",
    "                     .repeat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input placeholders\n",
    "X = Input(shape=[seq_len, n_seq], name='RealData')\n",
    "Z = Input(shape=[seq_len, n_seq], name='RandomData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN creation function\n",
    "def make_rnn(n_layers, hidden_units, output_units, name):\n",
    "    return Sequential([GRU(units=hidden_units,\n",
    "                           return_sequences=True,\n",
    "                           name=f'GRU_{i + 1}') for i in range(n_layers)] +\n",
    "                      [Dense(units=output_units,\n",
    "                             activation='sigmoid',\n",
    "                             name='OUT')], name=name)\n",
    "\n",
    "#creating the RNN's\n",
    "#embedder\n",
    "embedder = make_rnn(n_layers=3, \n",
    "                    hidden_units=hidden_dim, \n",
    "                    output_units=hidden_dim, \n",
    "                    name='Embedder')\n",
    "\n",
    "#recovery\n",
    "recovery = make_rnn(n_layers=3, \n",
    "                    hidden_units=hidden_dim, \n",
    "                    output_units=n_seq, \n",
    "                    name='Recovery')\n",
    "\n",
    "#generator and discriminator\n",
    "generator = make_rnn(n_layers=3, \n",
    "                     hidden_units=hidden_dim, \n",
    "                     output_units=hidden_dim, \n",
    "                     name='Generator')\n",
    "discriminator = make_rnn(n_layers=3, \n",
    "                         hidden_units=hidden_dim, \n",
    "                         output_units=1, \n",
    "                         name='Discriminator')\n",
    "supervisor = make_rnn(n_layers=2, \n",
    "                      hidden_units=hidden_dim, \n",
    "                      output_units=hidden_dim, \n",
    "                      name='Supervisor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training settings\n",
    "train_steps = 10000\n",
    "gamma = 1\n",
    "\n",
    "#generic loss functions\n",
    "mse = MeanSquaredError()\n",
    "bce = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RealData (InputLayer)       [(None, 30, 22)]          0         \n",
      "                                                                 \n",
      " Embedder (Sequential)       (None, 30, 24)            11256     \n",
      "                                                                 \n",
      " Recovery (Sequential)       (None, 30, 22)            11350     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,606\n",
      "Trainable params: 22,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Autoencoder summary: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [10:59<00:00, 15.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#autoencoder training\n",
    "H = embedder(X)\n",
    "X_tilde = recovery(H)\n",
    "autoencoder = Model(inputs=X,\n",
    "                    outputs=X_tilde,\n",
    "                    name='Autoencoder')\n",
    "print(f'Autoencoder summary: {autoencoder.summary()}')\n",
    "\n",
    "#autoencoder optimisation\n",
    "autoencoder_optimizer = Adam()\n",
    "\n",
    "#autoencoder training step\n",
    "@tf.function\n",
    "def train_autoencoder_init(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss_0, var_list)\n",
    "    autoencoder_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "#autoencoder trzining loop\n",
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = next(real_series_iter)\n",
    "    step_e_loss_t0 = train_autoencoder_init(X_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:31<00:00, 30.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#Supervised training\n",
    "\n",
    "#training optimiser\n",
    "supervisor_optimizer = Adam()\n",
    "\n",
    "#training step\n",
    "@tf.function\n",
    "def train_supervisor(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "    var_list = supervisor.trainable_variables\n",
    "    gradients = tape.gradient(g_loss_s, var_list)\n",
    "    supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return g_loss_s\n",
    "\n",
    "#training loop\n",
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = next(real_series_iter)\n",
    "    step_g_loss_s = train_supervisor(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AdversarialNetSupervised\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RandomData (InputLayer)     [(None, 30, 22)]          0         \n",
      "                                                                 \n",
      " Generator (Sequential)      (None, 30, 24)            11256     \n",
      "                                                                 \n",
      " Supervisor (Sequential)     (None, 30, 24)            7800      \n",
      "                                                                 \n",
      " Discriminator (Sequential)  (None, 30, 1)             10825     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,881\n",
      "Trainable params: 29,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Adverserial supervised summary: None\n",
      "Model: \"AdversarialNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RandomData (InputLayer)     [(None, 30, 22)]          0         \n",
      "                                                                 \n",
      " Generator (Sequential)      (None, 30, 24)            11256     \n",
      "                                                                 \n",
      " Discriminator (Sequential)  (None, 30, 1)             10825     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,081\n",
      "Trainable params: 22,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Adverserial emb summary: None\n"
     ]
    }
   ],
   "source": [
    "#joint training: Generator adversarial architecture - supervised\n",
    "E_hat = generator(Z)\n",
    "H_hat = supervisor(E_hat)\n",
    "Y_fake = discriminator(H_hat)\n",
    "adversarial_supervised = Model(inputs=Z,\n",
    "                               outputs=Y_fake,\n",
    "                               name='AdversarialNetSupervised')\n",
    "\n",
    "#plot_model(adversarial_supervised, show_shapes=True)\n",
    "print(f'Adverserial supervised summary: {adversarial_supervised.summary()}')\n",
    "\n",
    "#adverserial architecture in latent space\n",
    "Y_fake_e = discriminator(E_hat)\n",
    "adversarial_emb = Model(inputs=Z,\n",
    "                    outputs=Y_fake_e,\n",
    "                    name='AdversarialNet')\n",
    "\n",
    "#plot_model(adversarial_emb, show_shapes=True)\n",
    "print(f'Adverserial emb summary: {adversarial_emb.summary()}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SyntheticData\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RandomData (InputLayer)     [(None, 30, 22)]          0         \n",
      "                                                                 \n",
      " Generator (Sequential)      (None, 30, 24)            11256     \n",
      "                                                                 \n",
      " Supervisor (Sequential)     (None, 30, 24)            7800      \n",
      "                                                                 \n",
      " Recovery (Sequential)       (None, 30, 22)            11350     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,406\n",
      "Trainable params: 30,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Sythnetic data summary: None\n"
     ]
    }
   ],
   "source": [
    "#mean and variance loss\n",
    "X_hat = recovery(H_hat)\n",
    "synthetic_data = Model(inputs=Z,\n",
    "                       outputs=X_hat,\n",
    "                       name='SyntheticData')\n",
    "#plot_model(synthetic_data, show_shapes=True)\n",
    "print(f'Sythnetic data summary: {synthetic_data.summary()}')    \n",
    "\n",
    "\n",
    "def get_generator_moment_loss(y_true, y_pred):\n",
    "    y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n",
    "    y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n",
    "    g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
    "    g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
    "    return g_loss_mean + g_loss_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DiscriminatorReal\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " RealData (InputLayer)       [(None, 30, 22)]          0         \n",
      "                                                                 \n",
      " Embedder (Sequential)       (None, 30, 24)            11256     \n",
      "                                                                 \n",
      " Discriminator (Sequential)  (None, 30, 1)             10825     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,081\n",
      "Trainable params: 22,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Discriminator models summary: None\n"
     ]
    }
   ],
   "source": [
    "#discriminator\n",
    "#real data architecture\n",
    "Y_real = discriminator(H)\n",
    "discriminator_model = Model(inputs=X,\n",
    "                            outputs=Y_real,\n",
    "                            name='DiscriminatorReal')\n",
    "\n",
    "#plot_model(discriminator_model, show_shapes=True)\n",
    "print(f'Discriminator models summary: {discriminator_model.summary()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimisers\n",
    "generator_optimizer = Adam()\n",
    "discriminator_optimizer = Adam()\n",
    "embedding_optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator step training\n",
    "@tf.function\n",
    "def train_generator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_fake = adversarial_supervised(z)\n",
    "        generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),\n",
    "                                          y_pred=y_fake)\n",
    "        y_fake_e = adversarial_emb(z)\n",
    "        generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),\n",
    "                                            y_pred=y_fake_e)\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "        x_hat = synthetic_data(z)\n",
    "        generator_moment_loss = get_generator_moment_loss(x, x_hat)\n",
    "        generator_loss = (generator_loss_unsupervised +\n",
    "                          generator_loss_unsupervised_e +\n",
    "                          100 * tf.sqrt(generator_loss_supervised) +\n",
    "                          100 * generator_moment_loss)\n",
    "    var_list = generator.trainable_variables + supervisor.trainable_variables\n",
    "    gradients = tape.gradient(generator_loss, var_list)\n",
    "    generator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss\n",
    "\n",
    "#embedding train step\n",
    "@tf.function\n",
    "def train_embedder(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss, var_list)\n",
    "    embedding_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "#discriminator train step\n",
    "@tf.function\n",
    "def get_discriminator_loss(x, z):\n",
    "    y_real = discriminator_model(x)\n",
    "    discriminator_loss_real = bce(y_true=tf.ones_like(y_real),\n",
    "                                  y_pred=y_real)\n",
    "    y_fake = adversarial_supervised(z)\n",
    "    discriminator_loss_fake = bce(y_true=tf.zeros_like(y_fake),\n",
    "                                  y_pred=y_fake)\n",
    "    y_fake_e = adversarial_emb(z)\n",
    "    discriminator_loss_fake_e = bce(y_true=tf.zeros_like(y_fake_e),\n",
    "                                    y_pred=y_fake_e)\n",
    "    return (discriminator_loss_real +\n",
    "            discriminator_loss_fake +\n",
    "            gamma * discriminator_loss_fake_e)\n",
    "\n",
    "@tf.function\n",
    "def train_discriminator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        discriminator_loss = get_discriminator_loss(x, z)\n",
    "    var_list = discriminator.trainable_variables\n",
    "    gradients = tape.gradient(discriminator_loss, var_list)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 | d_loss: 2.0710 | g_loss_u: 0.6819 | g_loss_s: 0.0005 | g_loss_v: 0.3343 | e_loss_t0: 0.0567\n",
      " 1,000 | d_loss: 1.6145 | g_loss_u: 1.0555 | g_loss_s: 0.0000 | g_loss_v: 0.0505 | e_loss_t0: 0.0119\n",
      " 2,000 | d_loss: 1.7458 | g_loss_u: 0.9766 | g_loss_s: 0.0001 | g_loss_v: 0.0243 | e_loss_t0: 0.0126\n",
      " 3,000 | d_loss: 1.4082 | g_loss_u: 1.1271 | g_loss_s: 0.0002 | g_loss_v: 0.0941 | e_loss_t0: 0.0111\n",
      " 4,000 | d_loss: 1.6551 | g_loss_u: 1.4664 | g_loss_s: 0.0001 | g_loss_v: 0.0907 | e_loss_t0: 0.0109\n",
      " 5,000 | d_loss: 1.5184 | g_loss_u: 1.4200 | g_loss_s: 0.0001 | g_loss_v: 0.0381 | e_loss_t0: 0.0108\n",
      " 6,000 | d_loss: 1.6997 | g_loss_u: 1.2203 | g_loss_s: 0.0000 | g_loss_v: 0.0315 | e_loss_t0: 0.0106\n",
      " 7,000 | d_loss: 1.7667 | g_loss_u: 1.3464 | g_loss_s: 0.0001 | g_loss_v: 0.0396 | e_loss_t0: 0.0107\n",
      " 8,000 | d_loss: 1.8113 | g_loss_u: 1.2142 | g_loss_s: 0.0001 | g_loss_v: 0.0178 | e_loss_t0: 0.0100\n",
      " 9,000 | d_loss: 1.7873 | g_loss_u: 1.2495 | g_loss_s: 0.0000 | g_loss_v: 0.0263 | e_loss_t0: 0.0101\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\n",
    "for step in range(train_steps):\n",
    "    # Train generator (twice as often as discriminator)\n",
    "    for kk in range(2):\n",
    "        X_ = next(real_series_iter)\n",
    "        Z_ = next(random_series)\n",
    "        # Train generator\n",
    "        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n",
    "        # Train embedder\n",
    "        step_e_loss_t0 = train_embedder(X_)\n",
    "    X_ = next(real_series_iter)\n",
    "    Z_ = next(random_series)\n",
    "    step_d_loss = get_discriminator_loss(X_, Z_)\n",
    "    if step_d_loss > 0.15:\n",
    "        step_d_loss = train_discriminator(X_, Z_)\n",
    "    if step % 1000 == 0:\n",
    "        print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '\n",
    "              f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f}')\n",
    "\n",
    "#saving synthisizer\n",
    "synthetic_data.save('timegan_synthetic_data_synthisizer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate synthetic data\n",
    "generated_data = []\n",
    "for i in range(int(n_windows / batch_size)):\n",
    "    Z_ = next(random_series)\n",
    "    d = synthetic_data(Z_)\n",
    "    generated_data.append(d)\n",
    "\n",
    "generated_data = np.array(np.vstack(generated_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescaling\n",
    "generated_data = (scaler.inverse_transform(generated_data\n",
    "                                           .reshape(-1, n_seq))\n",
    "                  .reshape(-1, seq_len, n_seq))\n",
    "\n",
    "#getting generated data in same format as orig\n",
    "cols = real_data.columns\n",
    "m,n,r = generated_data.shape\n",
    "out_arr = np.column_stack((np.repeat(np.arange(m),n),generated_data.reshape(m*n,-1)))\n",
    "out_df = pd.DataFrame(out_arr)\n",
    "out_df.drop(out_df.columns[0],axis=1,inplace=True)\n",
    "out_df.columns = cols\n",
    "TG_generated_data = out_df\n",
    "TG_generated_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the data\n",
    "TG_generated_data['Date'] = pd.to_datetime(TG_generated_data['Date']).dt.normalize() #changing Date to datetime\n",
    "TG_generated_data = TG_generated_data.sort_values(by='Date') #ordering by date\n",
    "TG_generated_data.drop(['Unnamed: 0'], axis=1, inplace=True) #dropping unnamed column \n",
    "\n",
    "#saving\n",
    "TG_generated_data.to_csv(\"../Data/TimeGAN_synth_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth",
   "language": "python",
   "name": "synth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
